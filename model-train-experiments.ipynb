{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V_-JNdDY0B5"
      },
      "source": [
        "# Mineração de Texto - Experimentos e Treinamento do modelo de classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77O5kfsyqj-T"
      },
      "source": [
        "## Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBTaJCS67g-z"
      },
      "outputs": [],
      "source": [
        "# TODO: corrigir versão do pacote (incompatibilidade)\n",
        "%pip install numpy==2.2.2\n",
        "%pip install gensim\n",
        "%pip install datasets\n",
        "%pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uJtIHclTawh"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, ClassLabel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, AutoModel, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import string\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch.nn as nn\n",
        "import optuna\n",
        "from sklearn.pipeline import Pipeline as PipelineSkt\n",
        "\n",
        "from transformers import pipeline\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import logging\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import os\n",
        "import gc\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import joblib\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EwtS1habdpl"
      },
      "outputs": [],
      "source": [
        "# Altere para True a variavel abaixo para salvar os experimentos no Google Drive\n",
        "IS_LOGGIN_IN_DRIVE = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Altere essa variavel para testar cada estratégia de pré-processamento na etapa\n",
        "# de extração de padrões\n",
        "CURRENT_PROCESS_LEVEL = 'text_strategy_0'"
      ],
      "metadata": {
        "id": "-KmY5Z3THH04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Blw9k3Vg4Plx"
      },
      "outputs": [],
      "source": [
        "if IS_LOGGIN_IN_DRIVE:\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  now = datetime.now()\n",
        "  formatted_time = now.strftime('%Y-%m-%d%H:%M:%S')\n",
        "\n",
        "  folder_path = '/content/drive/MyDrive/mba-eng-de-software/experimentos'\n",
        "  if not os.path.exists(folder_path):\n",
        "      os.makedirs(folder_path)\n",
        "\n",
        "  log_file_path = f'{folder_path}/log_{formatted_time}.txt'\n",
        "\n",
        "  logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s %(message)s', force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbIWMtJ79iPM"
      },
      "outputs": [],
      "source": [
        "def log(to_log):\n",
        "  \"\"\"Salva o google drive e exibe no console\n",
        "\n",
        "  Args:\n",
        "    value: Texto para ser exibido.\n",
        "  \"\"\"\n",
        "  out = to_log\n",
        "  if not isinstance(out, str):\n",
        "    try:\n",
        "      out = str(to_log)\n",
        "    except Exception as e:\n",
        "      try:\n",
        "        out = to_log.to_string()\n",
        "      except Exception as e:\n",
        "        print(f\"Erro no log: {e}\")\n",
        "\n",
        "  if IS_LOGGIN_IN_DRIVE:\n",
        "    logging.info(out)\n",
        "  print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-ZXPEUNRsBo"
      },
      "outputs": [],
      "source": [
        "!gdown 1hTx4dTsgFc5NfK_VOW07gbZaBQ30nK9b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjKWk_TboH68"
      },
      "outputs": [],
      "source": [
        "!unzip portuguese_tweets_for_sentiment_analysis.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "387eIsn_qsUZ"
      },
      "source": [
        "## Parte 1: Seleção da Base de Dados e análises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSQxrLptTdCT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./TrainingDatasets/TrainTema.csv', sep=';', encoding='utf-8')\n",
        "df = df[['tweet_text', 'sentiment']] # capturando apenas as colunas que irão ser utilizadas\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7VniPPfTl7F"
      },
      "outputs": [],
      "source": [
        "# Analisando variável target\n",
        "df_target_count = df.sentiment.value_counts()\n",
        "df_target_percentage = df.sentiment.value_counts(normalize = True) * 100\n",
        "\n",
        "log(f'Textos classificados como positivos: {df_target_count[1]} ({(df_target_percentage[0]):.2f}%)')\n",
        "log(f'Textos classificados como negativos: {df_target_count[0]} ({(df_target_percentage[1]):.2f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji3PKPflWIGU"
      },
      "outputs": [],
      "source": [
        "# Analisando se possui algum dado nulo\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0nreeF0re44"
      },
      "outputs": [],
      "source": [
        "# Analisando se possui dados duplicados\n",
        "duplicated = df.duplicated()\n",
        "log(f'Número de linhas duplicadas: {duplicated.sum()}')\n",
        "log(f'dados duplicados: {df[duplicated]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mqhnWDcyWp1"
      },
      "outputs": [],
      "source": [
        "# Removendo dados duplicados\n",
        "df = df.drop_duplicates().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Uex94nqMYZn"
      },
      "outputs": [],
      "source": [
        "# Analisando variável target apos remocao da duplicação\n",
        "df_target_count = df.sentiment.value_counts()\n",
        "df_target_percentage = df.sentiment.value_counts(normalize = True) * 100\n",
        "\n",
        "log(f'Textos classificados como positivos: {df_target_count[1]} ({(df_target_percentage[0]):.2f}%)')\n",
        "log(f'Textos classificados como negativos: {df_target_count[0]} ({(df_target_percentage[1]):.2f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3VmIeSaveqe"
      },
      "outputs": [],
      "source": [
        "# Maior texto que existe no dataset\n",
        "max_text_length = df['tweet_text'].apply(len).max()\n",
        "\n",
        "max_text_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9lLoR3QsIUO"
      },
      "outputs": [],
      "source": [
        "# analisando textos com contexto negativo\n",
        "negation_texts = df[df['tweet_text'].str.contains(r'\\b(?:não|nao|nunca|nem)\\b', case=False, na=False)]\n",
        "negation_texts_count = negation_texts.sentiment.value_counts()\n",
        "log(negation_texts_count)\n",
        "negation_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6lTYSw4xSHu"
      },
      "source": [
        "### Para o Dataset de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eh5S0NMxT4b"
      },
      "outputs": [],
      "source": [
        "# df_test\n",
        "df_test = pd.read_csv('./TestDatasets/TestTema.csv', sep=';', encoding='utf-8')\n",
        "df_test = df_test[['tweet_text', 'sentiment']] # capturando apenas as colunas que irão ser utilizadas\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqbN2ymRxUiO"
      },
      "outputs": [],
      "source": [
        "# Analisando variável target\n",
        "df_test_target_count = df_test.sentiment.value_counts()\n",
        "df_test_target_percentage = df_test.sentiment.value_counts(normalize = True) * 100\n",
        "\n",
        "log(f'Textos de teste classificados como positivos: {df_test_target_count[1]} ({(df_test_target_percentage[0]):.2f}%)')\n",
        "log(f'Textos de teste classificados como negativos: {df_test_target_count[0]} ({(df_test_target_percentage[1]):.2f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3alqxR4hxfFk"
      },
      "outputs": [],
      "source": [
        "# Analisando se possui algum dado nulo\n",
        "df_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIbnwrfzx0Cp"
      },
      "outputs": [],
      "source": [
        "# Analisando se possui dados duplicados\n",
        "duplicated_df_test = df_test.duplicated()\n",
        "log(f'Número de linhas duplicadas no teste: {duplicated_df_test.sum()}')\n",
        "log(f'dados duplicadosno teste: {df_test[duplicated_df_test]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMMdxGsVx0cK"
      },
      "outputs": [],
      "source": [
        "# Removendo dados duplicados\n",
        "df_test = df_test.drop_duplicates().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2kIshvGx-b4"
      },
      "outputs": [],
      "source": [
        "# Analisando variável target\n",
        "df_test_target_count = df_test.sentiment.value_counts()\n",
        "df_test_target_percentage = df_test.sentiment.value_counts(normalize = True) * 100\n",
        "\n",
        "log(f'Textos de teste classificados como positivos: {df_test_target_count[1]} ({(df_test_target_percentage[0]):.2f}%)')\n",
        "log(f'Textos de teste classificados como negativos: {df_test_target_count[0]} ({(df_test_target_percentage[1]):.2f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLUxUw6A1X9r"
      },
      "source": [
        "## Pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UhdpMmiXCl-"
      },
      "outputs": [],
      "source": [
        "# função para fazer a remoção basica e textos específicos (pontuações, url, @, #..)\n",
        "def basic_cleaning(text):\n",
        "  s = str(text).lower() # tudo para caixa baixa\n",
        "  s = s.replace('\\n', ' ') # quebras de linha\n",
        "  s = re.sub(r'http\\S+', '', s) # url\n",
        "  s = re.sub(r'@\\w+', '', s) # \\@s\n",
        "  s = re.sub(r'#\\w+', '', s) # \\#s\n",
        "  s = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", s)\n",
        "  s = re.sub(r'\\s+', ' ', s).strip()\n",
        "  s = re.sub(r'(k{2,}|h{2,})', '', s) # risadas\n",
        "  s = re.sub(r'\\d+', '', s) # números\n",
        "  return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XoZ7-Ao25jF"
      },
      "outputs": [],
      "source": [
        "# função para remover stopwords\n",
        "stop_words = nltk.corpus.stopwords.words('portuguese') # obtem stopwords\n",
        "\n",
        "def remove_stopwords(text, domain_stopwords=[], keep_stopwords=[]):\n",
        "  tokens = word_tokenize(text) # obtem tokens\n",
        "  v = [i for i in tokens if (not i in stop_words and not i in domain_stopwords) or i in keep_stopwords] # remove stopwords\n",
        "  s = \"\"\n",
        "  for token in v:\n",
        "    s += token + \" \"\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUDOD_oz8QBA"
      },
      "outputs": [],
      "source": [
        "# função para fazer a radicalização das palavras\n",
        "stemmer = nltk.stem.RSLPStemmer() # stemming para portuguese\n",
        "\n",
        "def stemming(text):\n",
        "  tokens = word_tokenize(text) # obtem tokens\n",
        "  sentence_stem = ''\n",
        "  doc_text_stems = [stemmer.stem(i) for i in tokens]\n",
        "  for stem in doc_text_stems:\n",
        "    sentence_stem += stem + \" \"\n",
        "\n",
        "  return sentence_stem.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlPIOyZfyrbf"
      },
      "outputs": [],
      "source": [
        "def apply_preprocess(df):\n",
        "  df['text_strategy_0'] = df['tweet_text'].apply(basic_cleaning)\n",
        "  df['text_strategy_1'] = df['text_strategy_0'].apply(remove_stopwords, args=(['tt', 'rt'], ['não', 'nunca', 'nem', 'nao'],))\n",
        "  df['text_strategy_2'] = df['text_strategy_1'].apply(stemming)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oYQHrR6BSJD"
      },
      "outputs": [],
      "source": [
        "df = apply_preprocess(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctw5XtniymDX"
      },
      "outputs": [],
      "source": [
        "## Para o dataset de teste\n",
        "df_test = apply_preprocess(df_test)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm1sJgJ_I3tl"
      },
      "outputs": [],
      "source": [
        "text_hate_c = ' '.join(df[df['sentiment'] == 1]['text_strategy_1'])\n",
        "wordcloud_hate_c = WordCloud(background_color=\"white\", width=800, height=400).generate(text_hate_c)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud_hate_c, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem - Sentimento positivo - Estratégia 1')\n",
        "plt.show()\n",
        "\n",
        "text_non_hate_c = ' '.join(df[df['sentiment'] == 0]['text_strategy_1'])\n",
        "wordcloud_non_hate_c = WordCloud( background_color=\"white\", width=800, height=400).generate(text_non_hate_c)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud_non_hate_c, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem - Sentimento negativo - Estratégia 1')\n",
        "plt.show()\n",
        "\n",
        "tokens_label_1 = []\n",
        "tokens_label_0 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etP_n2sZT_38"
      },
      "outputs": [],
      "source": [
        "# verifica a ocorrencia das palavras\n",
        "counter_1 = Counter(text_hate_c.split())\n",
        "counter_0 = Counter(text_non_hate_c.split())\n",
        "\n",
        "most_common_1 = counter_1.most_common(200)\n",
        "most_common_0 = counter_0.most_common(200)\n",
        "\n",
        "log(f'Classe 1 contextualizada: {most_common_1}')\n",
        "log(f'Classe 0 contextualizada: {most_common_0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGSksO20Laon"
      },
      "outputs": [],
      "source": [
        "text_hate_sc = ' '.join(df[df['sentiment'] == 1]['text_strategy_2'])\n",
        "wordcloud_hate_sc = WordCloud(background_color=\"white\", width=800, height=400).generate(text_hate_sc)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud_hate_sc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem - Sentimento Positivo - Estratégia 2')\n",
        "plt.show()\n",
        "\n",
        "text_non_hate_sc = ' '.join(df[df['sentiment'] == 0]['text_strategy_2'])\n",
        "wordcloud_non_hate_sc = WordCloud( background_color=\"white\", width=800, height=400).generate(text_non_hate_sc)\n",
        "\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud_non_hate_sc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nuvem - Sentimento Negativo - Estratégia 2')\n",
        "plt.show()\n",
        "\n",
        "tokens_label_1 = []\n",
        "tokens_label_0 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK1Ob4x0L84o"
      },
      "outputs": [],
      "source": [
        "# verifica a ocorrencia das palavras\n",
        "counter_1 = Counter(text_hate_sc.split())\n",
        "counter_0 = Counter(text_non_hate_sc.split())\n",
        "\n",
        "most_common_1 = counter_1.most_common(200)\n",
        "most_common_0 = counter_0.most_common(200)\n",
        "\n",
        "log(f'Classe 1 contextualizada: {most_common_1}')\n",
        "log(f'Classe 0 contextualizada: {most_common_0}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xD5R8KbUQu7"
      },
      "source": [
        "## Parte 3 - Extração de padrões"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh-73XZM92ic"
      },
      "outputs": [],
      "source": [
        "log(f'Extraindo padrões utilizando: {CURRENT_PROCESS_LEVEL}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qnOdWIwXnGq"
      },
      "outputs": [],
      "source": [
        "df_current_process = df[[CURRENT_PROCESS_LEVEL, 'sentiment']].copy()\n",
        "df_current_process.columns = ['text', 'label']\n",
        "# Poda para ficar somente 10000 samples\n",
        "df_current_process = df_current_process.sample(n=10000, random_state=42)\n",
        "df_current_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6lAMXUO0Nqx"
      },
      "outputs": [],
      "source": [
        "# df_test\n",
        "df_test_current_process = df_test[[CURRENT_PROCESS_LEVEL, 'sentiment']].copy()\n",
        "df_test_current_process.columns = ['text', 'label']\n",
        "df_test_current_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Liq7MPY199f-"
      },
      "outputs": [],
      "source": [
        "def knn_classifier(X_train, y_train, X_test, y_test, experiment, scaller, classes, should_save=False):\n",
        "  log(f'Iniciando experimento {experiment}')\n",
        "\n",
        "  pipeline = PipelineSkt([\n",
        "      ('scaler', scaller),\n",
        "      ('knn', KNeighborsClassifier())\n",
        "  ])\n",
        "\n",
        "  param_grid = {\n",
        "      'knn__n_neighbors': [1, 3, 5, 7, 9],\n",
        "      'knn__metric': ['euclidean', 'cosine'],\n",
        "      'knn__weights': ['uniform', 'distance'],\n",
        "  }\n",
        "\n",
        "  grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "  grid.fit(X_train, y_train)\n",
        "\n",
        "  if should_save:\n",
        "    joblib.dump(grid, f'{experiment}.joblib')\n",
        "\n",
        "  best_index = grid.best_index_\n",
        "  best_mean_accuracy = grid.best_score_\n",
        "  best_std_accuracy = grid.cv_results_['std_test_score'][best_index]\n",
        "\n",
        "  log(f'Melhores parâmetros: {grid.best_params_}')\n",
        "  log(f'Acurácia: {best_mean_accuracy}')\n",
        "  log(f'Desvio padrão: {best_std_accuracy}')\n",
        "\n",
        "  y_pred = grid.predict(X_test)\n",
        "\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "  cr = classification_report(y_test, y_pred)\n",
        "\n",
        "  log('\\n\\n' + 'Report')\n",
        "  log('\\n\\n' + cr)\n",
        "  log('\\n\\n' + 'Matriz de confusão')\n",
        "  log('\\n\\n' + df_cm.to_string())\n",
        "  log('\\n\\n' + \"Acurácia: %0.4f, Desvio padrão: %0.4f\" % (best_mean_accuracy, best_std_accuracy))\n",
        "\n",
        "  log(f'Finalizando experimento {experiment}')\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='d')\n",
        "  plt.ylabel('Classe verdadeira')\n",
        "  plt.xlabel('Classe predita')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL6m8Pr4CpBb"
      },
      "outputs": [],
      "source": [
        "def rf_classifier(X_train, y_train, X_test, y_test, experiment, classes, should_save=False):\n",
        "  log(f'Iniciando experimento {experiment}')\n",
        "\n",
        "  pipeline = PipelineSkt([\n",
        "      ('rf', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
        "  ])\n",
        "\n",
        "  param_grid = {\n",
        "      'rf__n_estimators': [100, 150],\n",
        "      'rf__max_depth': [None, 10, 20],\n",
        "      'rf__min_samples_split': [2, 5],\n",
        "      'rf__min_samples_leaf': [1, 2],\n",
        "      'rf__bootstrap': [True]\n",
        "  }\n",
        "\n",
        "  grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "  grid.fit(X_train, y_train)\n",
        "\n",
        "  if should_save:\n",
        "    joblib.dump(grid, f'{experiment}.joblib')\n",
        "\n",
        "  best_index = grid.best_index_\n",
        "  best_mean_accuracy = grid.best_score_\n",
        "  best_std_accuracy = grid.cv_results_['std_test_score'][best_index]\n",
        "\n",
        "  log(f'Melhores parâmetros: {grid.best_params_}')\n",
        "  log(f'Acurácia: {best_mean_accuracy}')\n",
        "  log(f'Desvio padrão: {best_std_accuracy}')\n",
        "\n",
        "  y_pred = grid.predict(X_test)\n",
        "\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "  cr = classification_report(y_test, y_pred)\n",
        "\n",
        "  log('\\n\\n' + 'Report')\n",
        "  log('\\n\\n' + cr)\n",
        "  log('\\n\\n' + 'Matriz de confusão')\n",
        "  log('\\n\\n' + df_cm.to_string())\n",
        "  log('\\n\\n' + \"Acurácia: %0.4f, Desvio padrão: %0.4f\" % (best_mean_accuracy, best_std_accuracy))\n",
        "\n",
        "  log(f'Finalizando experimento {experiment}')\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='d')\n",
        "  plt.ylabel('Classe verdadeira')\n",
        "  plt.xlabel('Classe predita')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "356yfEi3WH0Q"
      },
      "source": [
        "#### Teste 0: Ponderação TF-IDF - Com classificador Knn e Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63evQNph88PN"
      },
      "outputs": [],
      "source": [
        "log('Teste 0: Ponderação TF-IDF - Com classificador Knn e Random Forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMvWC7uiXIhS"
      },
      "outputs": [],
      "source": [
        "df_train_t0 = df_current_process\n",
        "df_test_t0 = df_test_current_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGZf_6NxXL5e"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_XoVohHXVRe"
      },
      "outputs": [],
      "source": [
        "X_train = vectorizer.fit_transform(df_train_t0['text'].to_list())\n",
        "y_train = df_train_t0['label'].to_list()\n",
        "\n",
        "X_test = vectorizer.transform(df_test_t0['text'].to_list())\n",
        "y_true = df_test_t0['label'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4tbUm4zaMp"
      },
      "source": [
        "##### Usando KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0515QwLUXX0b"
      },
      "outputs": [],
      "source": [
        "knn_classifier(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  X_test,\n",
        "  y_true,\n",
        "  \"Teste 0: Ponderação TF-IDF - Com classificador Knn\",\n",
        "  MaxAbsScaler(),\n",
        "  ['Classe 0', 'Classe 1']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNf0b43yzg3e"
      },
      "source": [
        "##### Usando Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hddgHT40zt-F"
      },
      "outputs": [],
      "source": [
        "rf_classifier(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  X_test,\n",
        "  y_true,\n",
        "  \"Teste 0: Ponderação TF-IDF - Com classificador Random Forest\",\n",
        "  ['Classe 0', 'Classe 1']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHd6ezxisd-9"
      },
      "source": [
        "#### Teste 1: Word Embeddings Estática - Word2Vec - Com classificador Knn e Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHjDKIpl9Kyp"
      },
      "outputs": [],
      "source": [
        "log('Teste 1: Word Embeddings Estática - Word2Vec - Com classificador Knn e Random Forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_oV_zrb1EQD"
      },
      "outputs": [],
      "source": [
        "W2V_VECTOR_SIZE = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8A5-9zqiTkF"
      },
      "outputs": [],
      "source": [
        "df_current_process['tokens'] = df_current_process['text'].apply(word_tokenize)\n",
        "df_test_current_process['tokens'] = df_test_current_process['text'].apply(word_tokenize)\n",
        "w2v_model = gensim.models.Word2Vec(sentences=df_current_process['tokens'], vector_size=W2V_VECTOR_SIZE, window=5, min_count=1, workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg6IEc4ghv2c"
      },
      "outputs": [],
      "source": [
        "df_train_t1 = df_current_process\n",
        "df_test_t1 = df_test_current_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMjg2fL-z6VV"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(df):\n",
        "  doc_embeddings = []\n",
        "  for index, row in df.iterrows():\n",
        "      tokens = row['tokens']\n",
        "      L = []\n",
        "      for token in tokens:\n",
        "        try:\n",
        "            L.append(w2v_model.wv[token])\n",
        "        except KeyError:\n",
        "            print('Ocorreu um erro')\n",
        "            pass\n",
        "\n",
        "      if len(L) > 0:\n",
        "          text_vec = np.mean(np.array(L), axis=0)\n",
        "      else:\n",
        "          text_vec = np.zeros(W2V_VECTOR_SIZE)\n",
        "\n",
        "      doc_embeddings.append(text_vec)\n",
        "  return doc_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCuqi1DoinAd"
      },
      "outputs": [],
      "source": [
        "train_embeddings = get_embeddings(df_train_t1)\n",
        "test_embeddings = get_embeddings(df_test_t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI9-2o1zh-mi"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(train_embeddings)\n",
        "y_train = df_train_t1['label'].to_list()\n",
        "\n",
        "X_test = np.array(test_embeddings)\n",
        "y_true = df_test_t1['label'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahFyLxrQxwxy"
      },
      "source": [
        "##### Usando KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip7W-KBwi5lm"
      },
      "outputs": [],
      "source": [
        "knn_classifier(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  X_test,\n",
        "  y_true,\n",
        "  \"Teste 1: Word Embeddings Estática - Word2Vec - Com classificador Knn\",\n",
        "  MinMaxScaler(),\n",
        "  ['Classe 0', 'Classe 1']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KApKa8G0FtDA"
      },
      "source": [
        "##### Usando Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EZkp4NUFtDE"
      },
      "outputs": [],
      "source": [
        "rf_classifier(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  X_test,\n",
        "  y_true,\n",
        "  \"Teste 1: Word Embeddings Estática - Word2Vec - Com classificador Random Forest\",\n",
        "  ['Classe 0', 'Classe 1']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKbZ7GtLssxl"
      },
      "source": [
        "#### Teste 2: Word Embeddings Contextuais - Com classificador Knn e Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fYMkK6utn7K"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40FKlKbb9qWy"
      },
      "outputs": [],
      "source": [
        "log('Teste 2: Word Embeddings Contextuais - Com classificador Knn e Random Forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmfDWBXnGVVM"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aacDTCLHdEF"
      },
      "outputs": [],
      "source": [
        "df_current_process = df[[CURRENT_PROCESS_LEVEL, 'sentiment']].copy()\n",
        "df_current_process.columns = ['text', 'label']\n",
        "df_current_process\n",
        "\n",
        "# df_test\n",
        "df_test_current_process = df_test[[CURRENT_PROCESS_LEVEL, 'sentiment']].copy()\n",
        "df_test_current_process.columns = ['text', 'label']\n",
        "df_test_current_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZmtLMxsHVVF"
      },
      "outputs": [],
      "source": [
        "df_train_t2 = df_current_process\n",
        "df_test_t2 = df_test_current_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uwZmEdpGVrn"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En7WXR86HLVT"
      },
      "outputs": [],
      "source": [
        "def get_context_embeddings(texts, batch_size=32):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Loop em minibatches\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i : i + batch_size].tolist()\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            max_length=300\n",
        "        )\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Pega o embedding do [CLS] (ou seja, o primeiro token = index 0)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        all_embeddings.append(batch_embeddings.cpu())  # Se quiser retornar para CPU\n",
        "\n",
        "    # Concatena todos os embeddings numa única matriz\n",
        "    return torch.cat(all_embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV_RVx7nImMK"
      },
      "outputs": [],
      "source": [
        "train_outputs = get_context_embeddings(df_train_t2['text'], batch_size=32)\n",
        "test_outputs = get_context_embeddings(df_test_t2['text'], batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zysoheZdIq-k"
      },
      "outputs": [],
      "source": [
        "X_train = train_outputs\n",
        "y_train = df_train_t2['label'].to_list()\n",
        "\n",
        "X_test = test_outputs\n",
        "y_true = df_test_t2['label'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ukgrzWLJG-"
      },
      "source": [
        "##### Usando KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wV-Z1BNLJG_"
      },
      "outputs": [],
      "source": [
        "knn_classifier(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  X_test,\n",
        "  y_true,\n",
        "  \"Teste 1: Word Embeddings Estática - Word2Vec - Com classificador Knn\",\n",
        "  MinMaxScaler(),\n",
        "  ['Classe 0', 'Classe 1']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JW03CZfLJHA"
      },
      "source": [
        "##### Usando Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCN7O3JfLJHA"
      },
      "outputs": [],
      "source": [
        "# Comentado devido ao poder computacional\n",
        "# rf_classifier(\n",
        "#   X_train,\n",
        "#   y_train,\n",
        "#   X_test,\n",
        "#   y_true,\n",
        "#   \"Teste 2: Word Embeddings Contextuais - Com classificador Random Forest\",\n",
        "#   ['Classe 0', 'Classe 1']\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khml-P4H2bC1"
      },
      "source": [
        "#### Teste 3: Ajuste fino em um modelo pré treinado com o Trainer do Hugging Face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OCfvzo3QINL"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if 'train_outputs' in globals() or 'train_outputs' in locals():\n",
        "  del train_outputs\n",
        "if 'test_outputs' in globals() or 'train_outputs' in locals():\n",
        "  del test_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drp4kSAz-9WE"
      },
      "outputs": [],
      "source": [
        "log('Teste 3: Ajuste fino em um modelo pré treinado com o Trainer do Hugging Face')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_-S9ZrM2dnO"
      },
      "outputs": [],
      "source": [
        "df_lv_2_with_sw = df_current_process\n",
        "df_lv_2_with_sw.columns = ['text', 'label']\n",
        "df_lv_2_with_sw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VSMOqc14NuE"
      },
      "outputs": [],
      "source": [
        "df_lv_2_with_sw_ds =  Dataset.from_pandas(df_lv_2_with_sw.reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDX9rKow2sAP"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "log('Modelo: ' + MODEL_NAME)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uGsyMfW3VhT"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpi2vqn030Nz"
      },
      "outputs": [],
      "source": [
        "df_tokenize = df_lv_2_with_sw_ds.map(preprocess_function, batched=True)\n",
        "df_tokenize = df_tokenize.remove_columns([\"text\"])\n",
        "df_tokenize.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJYXTEJQ6T12"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBjDPGEpM2vS"
      },
      "outputs": [],
      "source": [
        "df_tokenize = df_tokenize.cast_column('label', ClassLabel(num_classes=len(set(df_tokenize['label']))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Para o teste"
      ],
      "metadata": {
        "id": "caQs4bVk4yEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_lv_2_with_sw = df_test_current_process\n",
        "df_test_lv_2_with_sw.columns = ['text', 'label']\n",
        "df_test_lv_2_with_sw"
      ],
      "metadata": {
        "id": "5dgD6hBz5iU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_lv_2_with_sw_ds =  Dataset.from_pandas(df_test_lv_2_with_sw.reset_index(drop=True))\n",
        "df_test_tokenize = df_test_lv_2_with_sw_ds.map(preprocess_function, batched=True)\n",
        "df_test_tokenize = df_test_tokenize.remove_columns([\"text\"])\n",
        "df_test_tokenize.set_format(\"torch\")\n",
        "df_test_tokenize = df_test_tokenize.cast_column('label', ClassLabel(num_classes=len(set(df_test_tokenize['label']))))"
      ],
      "metadata": {
        "id": "SP4eGzdW4pEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J29c2rWPWJjb"
      },
      "outputs": [],
      "source": [
        "dft_test_val = df_test_tokenize.train_test_split(test_size=0.30, seed=42, stratify_by_column='label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J622SmqzO2Ap"
      },
      "outputs": [],
      "source": [
        "train_ds = df_tokenize\n",
        "test_ds = dft_test_val['train']\n",
        "val_ds = dft_test_val['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mlAMTOjPISe"
      },
      "outputs": [],
      "source": [
        "log(f'Modelo: {MODEL_NAME}')\n",
        "log(f'Dataset inicial: {str(df_tokenize.shape)}')\n",
        "log(f'Dataset de treinamento: {str(train_ds.shape)}')\n",
        "log(f'Dataset de teste: {str(test_ds.shape)}')\n",
        "log(f'Dataset de validação: {str(val_ds.shape)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsUI6NBivUM-"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdY1ZP6UeQ52"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='binary')\n",
        "    recall = recall_score(labels, preds, average='binary')\n",
        "    f1 = f1_score(labels, preds, average='binary')\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "def optuna_hp_space(trial):\n",
        "    return {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True),\n",
        "        'num_train_epochs': trial.suggest_int('num_train_epochs', 2, 5),\n",
        "        'per_device_train_batch_size': trial.suggest_categorical('per_device_train_batch_size', [8, 16, 32]),\n",
        "        'weight_decay': trial.suggest_float('weight_decay', 0.0, 0.3),\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='optuna_results',\n",
        "    eval_strategy=\"epoch\",\n",
        "\n",
        "    save_strategy=\"no\",\n",
        "    # save_strategy=\"epoch\",\n",
        "    # load_best_model_at_end=True,\n",
        "    greater_is_better=True,\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "\n",
        "    logging_strategy=\"no\",\n",
        "    seed=42,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "train_labels = np.array(train_ds['label'])\n",
        "classes = np.unique(train_labels)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "class_weights = class_weights.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f'Quantidade de 0s e 1s: {np.bincount(train_labels)}')\n",
        "print(f'Pesos para as classes: {class_weights}')\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    class_weights=class_weights,\n",
        ")\n",
        "\n",
        "def compute_objective(metrics):\n",
        "    return metrics['eval_f1']\n",
        "\n",
        "best_trials = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"optuna\",\n",
        "    hp_space=optuna_hp_space,\n",
        "    n_trials=10,\n",
        "    compute_objective=compute_objective,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz6mwLmoaUDQ"
      },
      "outputs": [],
      "source": [
        "log('\\nMelhor configuração encontrada')\n",
        "log(best_trials.hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvD1kod8ZBn_"
      },
      "outputs": [],
      "source": [
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "\n",
        "train_labels = np.array(train_ds['label'])\n",
        "classes = np.unique(train_labels)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "class_weights = class_weights.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(skf.split(np.zeros(len(train_labels)), train_labels)):\n",
        "    log(f\"Iniciando Fold {fold + 1}/{n_splits}\")\n",
        "\n",
        "    train_dataset = train_ds.select(train_indices)\n",
        "    val_dataset = train_ds.select(val_indices)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=2\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"my-model-fold-{fold}\",\n",
        "        learning_rate=best_trials.hyperparameters['learning_rate'],\n",
        "        per_device_train_batch_size=best_trials.hyperparameters['per_device_train_batch_size'],\n",
        "        per_device_eval_batch_size=best_trials.hyperparameters['per_device_train_batch_size'],\n",
        "        num_train_epochs=best_trials.hyperparameters['num_train_epochs'],\n",
        "        weight_decay=best_trials.hyperparameters['weight_decay'],\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        # save_strategy=\"epoch\",\n",
        "        # load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_f1\",\n",
        "        save_total_limit=1,\n",
        "        seed=42,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        class_weights=class_weights,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    eval_result = trainer.evaluate()\n",
        "\n",
        "    accuracy_list.append(eval_result['eval_accuracy'])\n",
        "    precision_list.append(eval_result['eval_precision'])\n",
        "    recall_list.append(eval_result['eval_recall'])\n",
        "    f1_list.append(eval_result['eval_f1'])\n",
        "\n",
        "    log(f'\\n Resultados do Fold {fold + 1}:')\n",
        "    log(f\"\\n Acurácia: {eval_result['eval_accuracy']}\")\n",
        "    log(f\"\\n Precisão: {eval_result['eval_precision']}\")\n",
        "    log(f\"\\n Recall: {eval_result['eval_recall']}\")\n",
        "    log(f\"\\n F1 Score: {eval_result['eval_f1']}\")\n",
        "\n",
        "avg_accuracy = np.mean(accuracy_list)\n",
        "avg_precision = np.mean(precision_list)\n",
        "avg_recall = np.mean(recall_list)\n",
        "avg_f1 = np.mean(f1_list)\n",
        "std_accuracy = np.std(accuracy_list)\n",
        "\n",
        "log('Resultados da Validação Cruzada:')\n",
        "log(f'Acurácia Média: {avg_accuracy}')\n",
        "log(f'Desvio padrão da acurácia: {std_accuracy}')\n",
        "log(f'Precisão Média: {avg_precision}')\n",
        "log(f'Recall Médio: {avg_recall}')\n",
        "log(f'F1 Score Médio: {avg_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZjYEXIAcT5c"
      },
      "outputs": [],
      "source": [
        "# treinando o modelo final com os melhores hiperparametros\n",
        "\n",
        "train_labels = np.array(train_ds['label'])\n",
        "classes = np.unique(train_labels)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "class_weights = class_weights.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "log(f'Quantidade de 0s e 1s: {np.bincount(train_labels)}')\n",
        "log(f'Pesos para as classes: {class_weights}')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"melhor-modelo-{CURRENT_PROCESS_LEVEL}\",\n",
        "    learning_rate=best_trials.hyperparameters['learning_rate'],\n",
        "    per_device_train_batch_size=best_trials.hyperparameters['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=best_trials.hyperparameters['per_device_train_batch_size'],\n",
        "    num_train_epochs=best_trials.hyperparameters['num_train_epochs'],\n",
        "    weight_decay=best_trials.hyperparameters['weight_decay'],\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_f1\",\n",
        "    save_total_limit=1,\n",
        "    seed=42,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    class_weights=class_weights,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_ubh7bve_jO"
      },
      "outputs": [],
      "source": [
        "eval_result = trainer.evaluate()\n",
        "log(f'\\n Resultado do melhor modelo')\n",
        "log(eval_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbY2mzme15eG"
      },
      "outputs": [],
      "source": [
        "# avaliando ele no dataset de teste\n",
        "predictions_output = trainer.predict(test_ds)\n",
        "pred_logits = predictions_output.predictions\n",
        "y_true = predictions_output.label_ids\n",
        "y_pred = np.argmax(pred_logits, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y1DarSY15eH"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "classes = ['Classe 0', 'Classe 1']\n",
        "df_cm_t3 = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "cr_t3 = classification_report(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zinJpI8x15eH"
      },
      "outputs": [],
      "source": [
        "log(f'\\n Teste 3: Ajuste fino em um modelo pré treinado com o Trainer do Hugging Face')\n",
        "log(f'\\n Report')\n",
        "log(cr_t3)\n",
        "log(f'\\n Matriz de confusão')\n",
        "log(df_cm_t3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='d')\n",
        "plt.ylabel('Classe verdadeira')\n",
        "plt.xlabel('Classe predita')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UoqPJWxDYWIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJGjTw8qgp9x"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(f'melhor-modelo-{CURRENT_PROCESS_LEVEL}')\n",
        "tokenizer.save_pretrained(f'melhor-modelo-{CURRENT_PROCESS_LEVEL}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA01FUsohHOK"
      },
      "outputs": [],
      "source": [
        "if IS_LOGGIN_IN_DRIVE: # se ja ta salvando logs entao tem acesso o drive, entao só salva\n",
        "  !zip -r melhor-modelo-{CURRENT_PROCESS_LEVEL}.zip melhor-modelo-{CURRENT_PROCESS_LEVEL}\n",
        "  !cp -r melhor-modelo-{CURRENT_PROCESS_LEVEL}.zip /content/drive/MyDrive/mba-eng-de-software/experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tci94hQnTPxW"
      },
      "source": [
        "#### Verifica o desempenho de um classificador Dummy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0Uosmh0SAhs"
      },
      "outputs": [],
      "source": [
        "df_train_tdummy = df_current_process\n",
        "df_test_tdummy = df_test_current_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2vd9VupSAht"
      },
      "outputs": [],
      "source": [
        "X_train = df_train_tdummy['text'].to_list()\n",
        "y_train = df_train_tdummy['label'].to_list()\n",
        "\n",
        "X_test = df_test_tdummy['text'].to_list()\n",
        "y_true = df_test_tdummy['label'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PYj5NSkTZ-V"
      },
      "outputs": [],
      "source": [
        "texts = df_lv_2_with_sw['text'].to_list()\n",
        "labels = df_lv_2_with_sw['label'].to_list()\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy='stratified')\n",
        "dummy_clf.fit(X_train, y_train)\n",
        "print(classification_report(y_true, dummy_clf.predict(X_test)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}